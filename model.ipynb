{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras import layers, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               11075712  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11170250 (42.61 MB)\n",
      "Trainable params: 11170250 (42.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_path = \"C:\\\\Users\\\\14384\\\\Desktop\\\\Project\\\\resized_data\"\n",
    "\n",
    "# list of classes\n",
    "classes = ['bambalouni', 'brik', 'chakchouka', 'couscous', 'kafteji', 'lablabi', 'makroudh', 'marqa', 'mloukhia', 'tajine']\n",
    "\n",
    "\n",
    "def tunisianFoodCNN(input_shape, num_classes):\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten the feature maps\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add dense layers for classification\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer with num_classes units\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 10\n",
    "custom_cnn_model = tunisianFoodCNN(input_shape, num_classes)\n",
    "\n",
    "# Check the model summary\n",
    "custom_cnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224),dtype=np.float32):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image.astype(dtype) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for labels and file paths\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "# Function to get class index based on folder name\n",
    "def get_class_index(folder_name):\n",
    "    return classes.index(folder_name)\n",
    "\n",
    "# Iterate through all image files in the data directory\n",
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    class_index = get_class_index(class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        all_labels.append(to_categorical(class_index, num_classes=10))\n",
    "        all_data.append(load_and_preprocess_image(image_path))\n",
    "\n",
    "# Split your data into training, validation, and test sets as needed\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the label lists to numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " one-hot encoding technique to represent the labels for each image. In one-hot encoding, each class is represented by a vector of zeros, except for the index of the class, which is set to 1. For example, the label for the image of a bambalouni would be a vector of zeros, except for the index of the bambalouni class, which would be set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input_shape and num_classes\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 10\n",
    "\n",
    "# Create the custom CNN model using the function\n",
    "model = tunisianFoodCNN(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up model checkpoint to save the best weights\n",
    "checkpoint = ModelCheckpoint('custom_cnn_weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "673/673 [==============================] - 432s 639ms/step - loss: 2.2061 - accuracy: 0.1801 - val_loss: 2.0604 - val_accuracy: 0.2598\n",
      "Epoch 2/30\n",
      "673/673 [==============================] - 373s 555ms/step - loss: 1.8312 - accuracy: 0.3467 - val_loss: 1.9121 - val_accuracy: 0.2949\n",
      "Epoch 3/30\n",
      "673/673 [==============================] - 376s 559ms/step - loss: 1.3035 - accuracy: 0.5467 - val_loss: 2.1508 - val_accuracy: 0.3091\n",
      "Epoch 4/30\n",
      "673/673 [==============================] - 406s 603ms/step - loss: 0.6118 - accuracy: 0.7967 - val_loss: 2.7224 - val_accuracy: 0.3467\n",
      "Epoch 5/30\n",
      "673/673 [==============================] - 399s 593ms/step - loss: 0.2073 - accuracy: 0.9398 - val_loss: 4.0830 - val_accuracy: 0.3450\n",
      "Epoch 6/30\n",
      "673/673 [==============================] - 351s 522ms/step - loss: 0.1180 - accuracy: 0.9692 - val_loss: 4.9623 - val_accuracy: 0.3367\n",
      "Epoch 7/30\n",
      "673/673 [==============================] - 358s 532ms/step - loss: 0.0811 - accuracy: 0.9800 - val_loss: 5.1889 - val_accuracy: 0.3158\n",
      "Epoch 8/30\n",
      "673/673 [==============================] - 355s 527ms/step - loss: 0.0680 - accuracy: 0.9831 - val_loss: 5.7189 - val_accuracy: 0.3258\n",
      "Epoch 9/30\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 0.0658 - accuracy: 0.9817 - val_loss: 5.7204 - val_accuracy: 0.3450\n",
      "Epoch 10/30\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 0.0560 - accuracy: 0.9842 - val_loss: 6.0645 - val_accuracy: 0.3375\n",
      "Epoch 11/30\n",
      "673/673 [==============================] - 356s 528ms/step - loss: 0.0502 - accuracy: 0.9862 - val_loss: 5.6620 - val_accuracy: 0.3283\n",
      "Epoch 12/30\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 0.0586 - accuracy: 0.9850 - val_loss: 6.3188 - val_accuracy: 0.3275\n",
      "Epoch 13/30\n",
      "673/673 [==============================] - 353s 525ms/step - loss: 0.0528 - accuracy: 0.9850 - val_loss: 6.3737 - val_accuracy: 0.3300\n",
      "Epoch 14/30\n",
      "673/673 [==============================] - 351s 522ms/step - loss: 0.0382 - accuracy: 0.9879 - val_loss: 7.7860 - val_accuracy: 0.3266\n",
      "Epoch 15/30\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 0.0302 - accuracy: 0.9925 - val_loss: 6.6501 - val_accuracy: 0.3124\n",
      "Epoch 16/30\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 0.0483 - accuracy: 0.9884 - val_loss: 6.6464 - val_accuracy: 0.3450\n",
      "Epoch 17/30\n",
      "673/673 [==============================] - 353s 525ms/step - loss: 0.0364 - accuracy: 0.9893 - val_loss: 7.4239 - val_accuracy: 0.3358\n",
      "Epoch 18/30\n",
      "673/673 [==============================] - 340s 505ms/step - loss: 0.0490 - accuracy: 0.9855 - val_loss: 7.0522 - val_accuracy: 0.3317\n",
      "Epoch 19/30\n",
      "673/673 [==============================] - 329s 488ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 6.8496 - val_accuracy: 0.3325\n",
      "Epoch 20/30\n",
      "673/673 [==============================] - 320s 475ms/step - loss: 0.0296 - accuracy: 0.9922 - val_loss: 6.9000 - val_accuracy: 0.3283\n",
      "Epoch 21/30\n",
      "673/673 [==============================] - 319s 474ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 6.9811 - val_accuracy: 0.3333\n",
      "Epoch 22/30\n",
      "673/673 [==============================] - 317s 471ms/step - loss: 0.0430 - accuracy: 0.9867 - val_loss: 6.4470 - val_accuracy: 0.3216\n",
      "Epoch 23/30\n",
      "673/673 [==============================] - 316s 469ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 7.1598 - val_accuracy: 0.3200\n",
      "Epoch 24/30\n",
      "673/673 [==============================] - 318s 472ms/step - loss: 0.0237 - accuracy: 0.9932 - val_loss: 7.1207 - val_accuracy: 0.3409\n",
      "Epoch 25/30\n",
      "673/673 [==============================] - 317s 470ms/step - loss: 0.0345 - accuracy: 0.9906 - val_loss: 7.0052 - val_accuracy: 0.3342\n",
      "Epoch 26/30\n",
      "673/673 [==============================] - 317s 470ms/step - loss: 0.0318 - accuracy: 0.9908 - val_loss: 7.3723 - val_accuracy: 0.3166\n",
      "Epoch 27/30\n",
      "673/673 [==============================] - 314s 466ms/step - loss: 0.0243 - accuracy: 0.9938 - val_loss: 6.7500 - val_accuracy: 0.3392\n",
      "Epoch 28/30\n",
      "673/673 [==============================] - 314s 467ms/step - loss: 0.0103 - accuracy: 0.9958 - val_loss: 7.0679 - val_accuracy: 0.3417\n",
      "Epoch 29/30\n",
      "673/673 [==============================] - 316s 469ms/step - loss: 0.0064 - accuracy: 0.9965 - val_loss: 7.4666 - val_accuracy: 0.3367\n",
      "Epoch 30/30\n",
      "673/673 [==============================] - 315s 467ms/step - loss: 0.0630 - accuracy: 0.9826 - val_loss: 7.0011 - val_accuracy: 0.3133\n",
      "94/94 [==============================] - 15s 149ms/step - loss: 6.9022 - accuracy: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit(np.array(train_data), train_labels, batch_size=batch_size,\n",
    "                    epochs=epochs, validation_data=(np.array(val_data), val_labels),\n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(np.array(test_data), np.array(test_labels))\n",
    "\n",
    "# Save the model\n",
    "model.save('Cnn_model_tunisianFood.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(image_path):\n",
    "    image = load_and_preprocess_image(image_path)\n",
    "    predictions = model.predict(np.array([image]))\n",
    "    class_index = np.argmax(predictions)\n",
    "    class_name = classes[class_index]  # Assuming you have defined 'classes' as a list of class names\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL CLASS : couscous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n",
      "Predicted Class: brik\n"
     ]
    }
   ],
   "source": [
    "image_path= 'C:\\\\Users\\\\14384\\\\Desktop\\\\Project\\\\Predict\\\\testingphoto1REALkosksi.jpg'\n",
    "predicted_class = predict_class(image_path)\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL CLASS: lablabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Class: bambalouni\n"
     ]
    }
   ],
   "source": [
    "image_path1 = 'C:\\\\Users\\\\14384\\\\Desktop\\\\Project\\\\Predict\\\\testingphoto2REAL-lablebi.jpg'\n",
    "\n",
    "predicted_class1 = predict_class(image_path1)\n",
    "print(\"Predicted Class:\", predicted_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Class: mloukhia\n"
     ]
    }
   ],
   "source": [
    "image_path1 = 'C:\\\\Users\\\\14384\\\\Desktop\\\\Project\\\\Predict\\\\testingphoto3REALtajine.jpg'\n",
    "\n",
    "predicted_class1 = predict_class(image_path1)\n",
    "print(\"Predicted Class:\", predicted_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Class: mloukhia\n"
     ]
    }
   ],
   "source": [
    "image_path1 = 'C:\\\\Users\\\\14384\\\\Desktop\\\\Project\\\\Predict\\\\testingphoto4REALmarga.jpg'\n",
    "\n",
    "predicted_class1 = predict_class(image_path1)\n",
    "print(\"Predicted Class:\", predicted_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on new images\n",
    "# def predict(image_path):\n",
    "#     image = load_and_preprocess_image(image_path)\n",
    "#     predictions = model.predict(np.array([image]))\n",
    "#     return predictions\n",
    "# image_path1 = 'C:\\Users\\14384\\Desktop\\Project\\Predict\\testingphoto1REALkosksi.jpg'\n",
    "# predictions1 = predict(image_path)\n",
    "\n",
    "# # class probabilities\n",
    "# print(predictions1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
